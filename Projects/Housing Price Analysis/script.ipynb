{"cells":[{"metadata":{"_cell_guid":"9a07f745-e309-4131-9b14-5343ea5d04cc","_uuid":"77e33b33dd529d0f7c55bfe8347269377cb7d951"},"cell_type":"markdown","source":"# Table of Contents\n* Process of Machine Learning Predictions\n* Understand the problem (Housing Pricing Data Set)\n* Hypothesis Generation\n* Get Data\n* Data Exploration\n* Data Pre-Processing\n* Feature Engineering - Create new features\n* Model Training - XGBoost, Neural Network, Lasso\n* Model Evaluation"},{"metadata":{"_cell_guid":"2f877185-65da-4cf8-8baf-957ef7a2858b","_uuid":"7c04a87bb721ae054a6bc9ceac4d74379ac55104"},"cell_type":"markdown","source":"# Process of Machine Learning Predictions\n\n## 1. Understand the problem\nThe data set for this project has been taken from Kaggle's Housing Data Set Knowledge Competition. As mentioned above, the data set is simple. This project aims at predicting house prices (residential) in Ames, Iowa, USA. I believe this problem statement is quite self-explanatory and doesn't need more explanation. Hence, we move to the next step.\n\n## 2. Hypothesis Generation\nWell, this is going to be interesting. What factors can you think of right now which can influence house prices ? As you read this, I want you to write down your factors as well, then we can match them with the data set. Defining a hypothesis has two parts: Null Hypothesis (Ho) and Alternate Hypothesis(Ha). They can be understood as:\n\n**Ho** - There exists no impact of a particular feature on the dependent variable. \n\n**Ha** - There exists a direct impact of a particular feature on the dependent variable.\n\nBased on a decision criterion (say, 5% significance level), we always 'reject' or 'fail to reject' the null hypothesis in statistical parlance. Practically, while model building we look for probability (p) values. If p value < 0.05, we reject the null hypothesis. If p > 0.05, we fail to reject the null hypothesis. Some factors which I can think of that directly influence house prices are the following:\n\n* Area of House\n* How old is the house\n* Location of the house\n* How close/far is the market\n* Connectivity of house location with transport\n* How many floors does the house have\n* What material is used in the construction\n* Water /Electricity availability\n* Play area / parks for kids (if any)\n* If terrace is available\n* If car parking is available\n* If security is available\n\n …keep thinking. I am sure you can come up with many more apart from these.\n"},{"metadata":{"_cell_guid":"aa00d025-e90a-4411-a5c0-daa52c183f2e","_uuid":"7c5b6d40f00d0eb79ff0d80e9639a903262b748c"},"cell_type":"markdown","source":"## 3. Get Data\nCheck the competition page where all the details about the data and variables are given. The data set consists of 81 explanatory variables. Yes, it's going to be one heck of a data exploration ride. But, we'll learn how to deal with so many variables. The target variable is SalePrice. As you can see the data set comprises numeric, categorical, and ordinal variables. Without further ado, let's start with hands-on coding.\n\n## 4. Data Exploration\nData Exploration is the key to getting insights from data. Practitioners say a good data exploration strategy can solve even complicated problems in a few hours. A good data exploration strategy comprises the following:\n\n* **Univariate Analysis** - It is used to visualize one variable in one plot. Examples: histogram, density plot, etc.\n* **Bivariate Analysis** - It is used to visualize two variables (x and y axis) in one plot. Examples: bar chart, line chart, area chart, etc.\n* **Multivariate Analysis** - As the name suggests, it is used to visualize more than two variables at once. Examples: stacked bar chart, dodged bar chart, etc.\n* **Cross Tables** -They are used to compare the behavior of two categorical variables (used in pivot tables as well).\n\nLet's load the necessary libraries and data and start coding."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"77c84f60-c3f4-4686-9fe5-100e9602f802","_uuid":"550d9bf623efd8701aac158f11cba0a072c1875a"},"cell_type":"markdown","source":"> # Loading libraries "},{"metadata":{"collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np                      # for matrix calculations\nimport pandas as pd                     # for easy data frame manipulation\nimport matplotlib.pyplot as plt         # for plotting data\n# to show plot inside python notebook\n%matplotlib inline                      \nplt.rcParams['figure.figsize'] = (10.0, 8.0)\nimport seaborn as sns                   # for beautiful plots\nfrom scipy import stats                 # to calculate mode, skew and ANOVA, etc.","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"4a183d21-bb27-4fb0-a51e-cf28ef8a8177","_uuid":"e4d858edf38e97260b17bdceb31f8ca26d421915"},"cell_type":"markdown","source":"> # Loading data"},{"metadata":{"_cell_guid":"657cf273-8afd-4fcd-8fdb-19926ab551af","_uuid":"ca13a9c0710b1f0b7f9a0ae2792d7367b402f69c","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(train['SalePrice'].head)\n","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"c13bb351-bf61-4839-959e-822b04c5efd8","_uuid":"e8af594bc65e9b7ee5029f8107cae61578abc24e","trusted":true},"cell_type":"code","source":"train.head()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"23121ee7-72ad-414c-b4ce-f0d1503badf5","_uuid":"554315718120733c68741510af34132883d8a1f0","trusted":false,"collapsed":true},"cell_type":"code","source":"print ('The train data has {0} rows and {1} columns'.format(train.shape[0],train.shape[1]))\nprint ('----------------------------')\nprint ('The test data has {0} rows and {1} columns'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"17d078b7-2d4d-4cf6-82c1-de090b90df6a","_uuid":"90aababad3d550a277ae908515ac3115b0179dc2","trusted":false},"cell_type":"code","source":"#missing value counts in each of these columns\nmiss = train.isnull().sum()/len(train)\nmiss = miss[miss > 0]\nmiss.sort_values(inplace=True) # inplace = True, doesn't make a copy of the original df (miss)\n#miss","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e7a7e8dd-73bf-44f2-873e-6625f2fd252a","_uuid":"e9e9d1388c0e0d014e08c2a82bb9f0a66b4ce6dd","trusted":false,"collapsed":true},"cell_type":"code","source":"#visualising missing values\nmiss = miss.to_frame() # Convert panda series to df\nmiss.columns = ['count'] # give column name to column 1\nmiss.index.names = ['Name'] # give column names to rownames\nmiss['Name'] = miss.index # create a new column with rownames\n\n#plot the missing value count\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.barplot(x = 'Name', y = 'count', data=miss)\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b464b655-d923-4b91-b508-105cb3e64301","_uuid":"b754fe96f564feb7eb90d9c5ab0d83f95aacb9dc","trusted":false,"collapsed":true},"cell_type":"code","source":"#SalePrice\nsns.distplot(train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"075512f4-72e0-4880-8a0f-72330b80f240","_uuid":"5abae41e18f3cb4078f032706e94a01a9f246e12","trusted":false,"collapsed":true},"cell_type":"code","source":"#skewness\nprint(\"The skewness of SalePrice is {}\".format(train['SalePrice'].skew()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f17ec3e4-79ba-4df0-98a4-4ef2f3da3f21","_uuid":"a064cc38338bfcb92ae72eaa45c92643c3a73b7c"},"cell_type":"markdown","source":"Let's log transform this variable and see if this variable distribution can get any closer to normal. "},{"metadata":{"_cell_guid":"a61399d2-1794-44aa-8de5-07ff7397e9de","_uuid":"0883cf6a972f26f813b254fcd02eeacb1cd150cf","trusted":false,"collapsed":true},"cell_type":"code","source":"#now transforming the target variable\ntarget = np.log(train['SalePrice']) # Natural log transformation\nprint ('Skewness is', target.skew())\nsns.distplot(target)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d501c35c-fad9-441e-ba5a-cf37e9623255","_uuid":"5c72e567cb97dfd88a5aafe45d1426b94d895569"},"cell_type":"markdown","source":"As you saw, log transformation of the target variable has helped us fixing its skewed distribution and the new distribution looks closer to normal. Since we have 80 variables, visualizing one by one wouldn't be an astute approach. Instead, we'll look at some variables based on their correlation with the target variable. However, there's a way to plot all variables at once, and we'll look at it as well. Moving forward, we'll separate numeric and categorical variables and explore this data from a different angle."},{"metadata":{"_cell_guid":"d774a657-7d8b-4de4-bead-a5225420bb1b","_uuid":"62f983c250dc4a99f9f528da90c0f2694d8ea0a5","trusted":false,"collapsed":true},"cell_type":"code","source":"#separate variables into new data frames\nnumeric_data = train.select_dtypes(include=[np.number])\ncat_data = train.select_dtypes(exclude=[np.number])\nprint (\"There are {} numeric and {} categorical columns in train data\".format(numeric_data.shape[1],cat_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"480391cc-093b-4593-9b8e-4ebd4383b936","_uuid":"73aff49c5d451565b80b6135fbe12000cee7f0ae","trusted":false},"cell_type":"code","source":"del numeric_data['Id']","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"88c6722f-a24d-469d-8887-d696e0eb2a56","_uuid":"a5cd684c856162ee379a1050eaaae27ae246a914"},"cell_type":"markdown","source":"Now, we are interested to learn about the correlation behavior of numeric variables. Out of 38 variables, I presume some of them must be correlated. If found, we can later remove these correlated variables as they won't provide any useful information to the model."},{"metadata":{"collapsed":true,"_cell_guid":"d2010468-edea-46a4-b47c-779aca0e841f","_uuid":"d130065d06ffef4bacf8022f715b64485adce403","trusted":false},"cell_type":"code","source":"#correlation plot\n\n#sns.palplot(sns.color_palette(\"Set2\",10))\n#sns.set_palette(\"hls\")\ncmap = sns.cubehelix_palette(n_colors=100, start=2, rot=1, light=0.9, dark=0.2)\n\ncorr = numeric_data.corr()\nsns.heatmap(corr,cmap=cmap)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed2fece0-d560-45c4-8de5-8f0549884d64","_uuid":"3d663b68daf60410180a7dca3689f9cd98f520a4"},"cell_type":"markdown","source":"Notice the last row of this map. We can see the correlation of all variables against SalePrice. As you can see, some variables seem to be strongly correlated with the target variable. Here, a numeric correlation score will help us understand the graph better."},{"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"517792bb-dbfe-43e8-8c4e-f5cad7cfd66c","_uuid":"faf8c99f9cff619809e03afcef3171e3b1ea2eb2","trusted":false},"cell_type":"code","source":"sns.pairplot(numeric_data.fillna(numeric_data.median())\n             ,x_vars=numeric_data.columns[:9]\n             , y_vars=numeric_data.columns[-1]\n             ,size=5, kind=\"reg\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"0791285f-1a85-490d-9e1e-aebb9cf36380","_uuid":"28ae4e0027dcab794daaead400acf060c72e92b9","trusted":false},"cell_type":"code","source":"print (corr['SalePrice'].sort_values(ascending=False)[:15], '\\n') #top 15 values\nprint ('----------------------')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:]) #last 5 values","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"73820326-c06d-4639-b8d3-7cfdaad77bb9","_uuid":"946fed8424abd20f3c9a1eb1c5bbfdc8a4edd376"},"cell_type":"markdown","source":"Here we see that the OverallQual feature is 79% correlated with the target variable. Overallqual feature refers to the overall material and quality of the materials of the completed house. Well, this make sense as well. People usually consider these parameters for their dream house. In addition, GrLivArea is 70% correlated with the target variable. GrLivArea refers to the living area (in sq ft.) above ground. The following variables show people also care about if the house has a garage, the area of that garage, the size of the basement area, etc.\nLet's check the OverallQual variable in detail."},{"metadata":{"collapsed":true,"_cell_guid":"3b408caa-6f12-4110-a920-be33408c4a8b","_uuid":"516a6e9a327cc04113725faa9780232490b3a521","trusted":false},"cell_type":"code","source":"train['OverallQual'].unique()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"6608694a-127a-435c-95d4-d60c27e484ef","_uuid":"a57984e940f931aa6603c4231cee02a3e12602db"},"cell_type":"markdown","source":"The overall quality is measured on a scale of 1 to 10. Hence, we can fairly treat it as an ordinal variable. An ordinal variable has an inherent order. For example, Rank of students in class, data collected on Likert scale, etc. Let's check the median sale price of houses with respect to OverallQual. You might be wondering, “Why median ?” We are using median because the target variable is skewed. A skewed variable has outliers and median is robust to outliers.\n\nWe can create such aggregated tables using pandas pivot tables quite easily."},{"metadata":{"collapsed":true,"_cell_guid":"f63dc4a3-5cd0-4f5d-8eeb-9ea9218945bc","_uuid":"45a91f8a14adc95dd387c75321dabcd785de62bc","trusted":false},"cell_type":"code","source":"#let's check the mean price per quality and plot it.\npivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\npivot","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"168920bb-a63b-426a-8a89-90ac465dbe83","_uuid":"1564d9430b0946f40d38f1eb229e390588a07676"},"cell_type":"markdown","source":"Let's plot this table and understand the median behavior using a bar graph."},{"metadata":{"collapsed":true,"_cell_guid":"f5e1cb25-f864-4a65-9a3c-e89818014aef","_uuid":"2068bbef69b53a3ab919bb5a298f649b1c76b863","trusted":false},"cell_type":"code","source":"pivot.plot(kind='bar', color='red')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"9cda0b8e-ab06-4669-8241-079dd4a0ed33","_uuid":"fab0c1efe365ae33140ab405a9fcc467b9dcea08"},"cell_type":"markdown","source":"This behavior is quite normal. As the overall quality of a house increases, its sale price also increases. Let's visualize the next correlated variable GrLivArea and understand their behavior.\n"},{"metadata":{"_kg_hide-input":false,"collapsed":true,"_kg_hide-output":false,"_cell_guid":"06cacffd-ae5f-42a7-9791-433e1a4dcf12","_uuid":"f7b5a44af209689cff957636233c4fe23833c763","trusted":false},"cell_type":"code","source":"#GrLivArea variable\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bab33e57-8f78-4d7b-81dc-c8d813421e63","_uuid":"de8cd0cbd67888372efb1b7cec3038c7bed5b64c"},"cell_type":"markdown","source":"As seen above, here also we see a direct correlation of living area with sale price. However, we can spot an outlier value GrLivArea > 5000. I've seen outliers play a significant role in spoiling a model's performance. Hence, we'll get rid of it. If you are enjoying this activity, you can visualize other correlated variables as well. Now, we'll move forward and explore categorical features. The simplest way to understand categorical variables is using .describe() command."},{"metadata":{"collapsed":true,"_cell_guid":"d825e696-fc92-4ccf-b45a-7d0560c92c14","_uuid":"d3d3f592102cdcbd5e369457ef1c5ecf6ddc0cac","trusted":false},"cell_type":"code","source":"cat_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a2f27482-659d-42f2-97e9-ed47b8bad36c","_uuid":"f8bc83a56ef6f10ae5e5f490ee3fe83ff5c9cfc4"},"cell_type":"markdown","source":"Let's check the median sale price of a house based on its SaleCondition. SaleCondition explains the condition of sale. Not much information is given about its categories."},{"metadata":{"collapsed":true,"_cell_guid":"2b2448dc-2e8c-4b80-ab4f-7ff5d9d40b82","_uuid":"ae92f5286eb9108776adf4da1260b32f69d63f01","trusted":false},"cell_type":"code","source":"sp_pivot = train.pivot_table(index='SaleCondition', values='SalePrice', aggfunc=np.median)\nsp_pivot","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f9bba129-d5fd-4462-920a-3b61567b5d8d","_uuid":"fac64e7595ea3287140aeb9c6803adbbc9d61f50","trusted":false},"cell_type":"code","source":"sp_pivot.plot(kind='bar',color='red')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49e3a2d5-118f-44e8-84a7-eeedabfe6eba","_uuid":"aa63eb83d1c8d1bcf7835b0b41c4ac4632877f37"},"cell_type":"markdown","source":"We see that SaleCondition Partial has the highest mean sale price. Though, due to lack of information we can't generate many insights from this data. Moving forward, like we used correlation to determine the influence of numeric features on SalePrice. Similarly, we'll use the ANOVA test to understand the correlation between categorical variables and SalePrice. ANOVA test is a statistical technique used to determine if there exists a significant difference in the mean of groups. For example, let's say we have two variables A and B. Each of these variables has 3 levels (a1,a2,a3 and b1,b2,b3). If the mean of these levels with respect to the target variable is the same, the ANOVA test will capture this behavior and we can safely remove them."},{"metadata":{"_cell_guid":"17d2b57e-a8b0-48ab-aef6-82b18c398565","_uuid":"cd2b4d9eee1a63532348a477be0a8cfe2557be85"},"cell_type":"markdown","source":"While using ANOVA, our hypothesis is as follows:\n\nHo - There exists no significant difference between the groups. Ha - There exists a significant difference between the groups.\n\nNow, we'll define a function which calculates p values. From those p values, we'll calculate a disparity score. Higher the disparity score, better the feature in predicting sale price."},{"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"1f4ae4f1-01a5-4f7a-b382-454cc5d0403b","_uuid":"0314abb38369d854632d2fa2d948001342d6b83e","trusted":false},"cell_type":"code","source":"cat = [f for f in train.columns if train.dtypes[f] == 'object']\ndef anova(frame):\n    anv = pd.DataFrame()\n    anv['features'] = cat\n    pvals = []\n    for c in cat:\n           samples = []\n           for cls in frame[c].unique():\n                  s = frame[frame[c] == cls]['SalePrice'].values\n                  samples.append(s)\n           pval = stats.f_oneway(*samples)[1]\n           pvals.append(pval)\n    anv['pval'] = pvals\n    return anv.sort_values('pval')\n\ncat_data['SalePrice'] = train.SalePrice.values\nk = anova(cat_data) \nk['disparity'] = np.log(1./k['pval'].values) \nsns.barplot(data=k, x = 'features', y='disparity') \nplt.xticks(rotation=90) \nplt ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33c8b0d3-ccae-46ad-9acf-50345c9e14f3","_uuid":"e40b8035bac60a27811406f8c6dc2d602f71c7c8"},"cell_type":"markdown","source":"Here we see that among all categorical variablesNeighborhoodturned out to be the most important feature followed by ExterQual, KitchenQual, etc. It means that people also consider the goodness of the neighborhood, the quality of the kitchen, the quality of the material used on the exterior walls, etc. Finally, to get a quick glimpse of all variables in a data set, let's plot histograms for all numeric variables to determine if all variables are skewed. For categorical variables, we'll create a boxplot and understand their nature."},{"metadata":{"collapsed":true,"_cell_guid":"a4f0d8ef-3b55-455a-a743-07731c61b26d","_uuid":"b539529a46e888f26b27a55ad7457f54426de099","trusted":false},"cell_type":"code","source":"#create numeric plots\nnum = [f for f in train.columns if train.dtypes[f] != 'object']\nnum.remove('Id')\nnd = pd.melt(train, value_vars = num)\nn1 = sns.FacetGrid (nd, col='variable', col_wrap=4, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\nn1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7b9e5ba7-02b8-4498-b09d-f804688aeb27","_uuid":"3757f378da9df5e82ba235312de9ca80595f8f7b"},"cell_type":"markdown","source":"As you can see, most of the variables are right skewed. We'll have to transform them in the next stage. Now, let's create boxplots for visualizing categorical variables."},{"metadata":{"collapsed":true,"_cell_guid":"43b3a54d-eb60-4ad6-9b24-d65193d5a7d1","_uuid":"81f29b28372205ed65f131ae6d0b0fe0c22bc4b3","trusted":false},"cell_type":"code","source":"def boxplot(x,y,**kwargs):\n            sns.boxplot(x=x,y=y)\n            x = plt.xticks(rotation=90)\n\ncat = [f for f in train.columns if train.dtypes[f] == 'object']\n\np = pd.melt(train, id_vars='SalePrice', value_vars=cat)\ng = sns.FacetGrid (p, col='variable', col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, 'value','SalePrice')\ng","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c9a21206-d466-4ebd-a00d-037705718c05","_uuid":"3024b561e27b5a25dd91f5ab1dc3fab3e4f78c94"},"cell_type":"markdown","source":"Here, we can see that most of the variables possess outlier values. It would take us days if we start treating these outlier values one by one. Hence, for now we'll leave them as is and let our algorithm deal with them. As we know, tree-based algorithms are usually robust to outliers."},{"metadata":{"_cell_guid":"d515a137-8846-4051-9a7f-14d403ae22f6","_uuid":"785577dd9b285476591d48439abe8cfaf1f6d766"},"cell_type":"markdown","source":"# Data Pre-Processing"},{"metadata":{"_cell_guid":"342709ca-ad71-43e6-a809-9aa0c3ad784d","_uuid":"b3faf58b6c359db6d7c777f06f3d667f3f663677"},"cell_type":"markdown","source":"In this stage, we'll deal with outlier values, encode variables, impute missing values, and take every possible initiative which can remove inconsistencies from the data set. If you remember, we discovered that the variable GrLivArea has outlier values. Precisely, one point crossed the 4000 mark. Let's remove that:"},{"metadata":{"collapsed":true,"_cell_guid":"7d1721cd-9127-49db-9410-18785605062b","_uuid":"1a0d3b240c0377a6a0e6791d189a82d93bd06fcf","trusted":false},"cell_type":"code","source":"#removing outliers\ntrain.drop(train[train['GrLivArea'] > 4000].index, inplace=True)\ntrain.shape #removed 4 rows","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3edfb22b-08e0-4cf5-986e-49b541ce8ce5","_uuid":"a5fb46e12c2084f3f1309f9602a966f44a693dc6"},"cell_type":"markdown","source":"In row 666, in the test data, it was found that information in variables related to 'Garage' (GarageQual, GarageCond, GarageFinish, GarageYrBlt) is missing. Let's impute them using the mode of these respective variables."},{"metadata":{"collapsed":true,"_cell_guid":"0e44701c-8c5c-4a23-9c35-9eb2b41bc662","_uuid":"35baedd330dba1870cae5fdbfc08f09b5c80a918","trusted":false},"cell_type":"code","source":"#imputing using mode\ntest.loc[666, 'GarageQual'] = \"TA\" #stats.mode(test['GarageQual']).mode\ntest.loc[666, 'GarageCond'] = \"TA\" #stats.mode(test['GarageCond']).mode\ntest.loc[666, 'GarageFinish'] = \"Unf\" #stats.mode(test['GarageFinish']).mode\ntest.loc[666, 'GarageYrBlt'] = \"1980\" #np.nanmedian(test['GarageYrBlt'])` ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f31dc8a2-b017-420d-9c00-76c06462d937","_uuid":"3bd9986433d5db45d77dc375ab249f6cec391fb1"},"cell_type":"markdown","source":"In row 1116, in test data, all garage variables are NA except GarageType. Let's mark it NA as well."},{"metadata":{"collapsed":true,"_cell_guid":"e694363e-5dce-4a0c-ad28-1d0d46629810","_uuid":"f549d9e3a78686b37eeb871d61a87d3744e31adf","trusted":false},"cell_type":"code","source":"#mark as missing\ntest.loc[1116, 'GarageType'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"121b7f65-3a6d-4ecc-918f-c1b907a63727","_uuid":"e85aa89aead848a990b36796e9b136039d76695e"},"cell_type":"markdown","source":"Now, we'll encode all the categorical variables. This is necessary because most ML algorithms do not accept categorical values, instead they are expected to be converted to numerical. LabelEncoder function from sklearn is used to encode variables. Let's write a function to do this:"},{"metadata":{"collapsed":true,"_cell_guid":"5470f2cf-cdbb-4b25-806c-b962dfcb2065","_uuid":"32b1d0c04a5ff2bf81a0406042f057fa7b655eaa","trusted":false},"cell_type":"code","source":"#importing function\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndef factorize(data, var, fill_na = None):\n      if fill_na is not None:\n            data[var].fillna(fill_na, inplace=True)\n      le.fit(data[var])\n      data[var] = le.transform(data[var])\n      return data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"621c320d-097c-4d5a-8444-2d65952ab025","_uuid":"9473d3dbc23e3f8700990fc12f3e31e1d2fc161f"},"cell_type":"markdown","source":"This function imputes the blank levels with mode values. The mode values are to be entered manually. Now, let's impute the missing values in LotFrontage variable using the median value of LotFrontage by Neighborhood. Such imputation strategies are built during data exploration. I suggest you spend some more time on data exploration. To do this, we should combine our train and test data so that we can modify both the data sets at once. Also, it'll save our time."},{"metadata":{"collapsed":true,"_cell_guid":"d3b80566-ab3c-49be-afaa-8b24cba8d1d5","_uuid":"2573128b4974ba73a5f9dc380aa0eb476cf17b55","trusted":false},"cell_type":"code","source":"#combine the data set\nalldata = train.append(test)\nalldata.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ecab0a9f-977d-464d-aae3-893e39a699c2","_uuid":"dd0b4ad164eb895cb2a93be5c56ab283b3ee33a0"},"cell_type":"markdown","source":"The combined data set has 2915 rows and 81 columns. Now, we'll impute the LotFrontage variable."},{"metadata":{"collapsed":true,"_cell_guid":"ea66cee2-5b27-4261-815e-962935806cd8","_uuid":"aaaa28f56920295fd6782de49119e2981786aad4","trusted":false},"cell_type":"code","source":"#impute lotfrontage by median of neighborhood\n# lotfrontage - Linear feet of street connected to property\nlot_frontage_by_neighborhood = train['LotFrontage'].groupby(train['Neighborhood'])\n\nfor key, group in lot_frontage_by_neighborhood:\n                # Get the index of the key with null values\n                idx = (alldata['Neighborhood'] == key) & (alldata['LotFrontage'].isnull())\n                # Impute with median\n                alldata.loc[idx, 'LotFrontage'] = group.median()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a8d544ba-344a-434a-a42c-bdd1ba827f3f","_uuid":"a41bcbb31d7e721f7b6ea6816e5f699a257bba86"},"cell_type":"markdown","source":"Next, in other numeric variables, we'll impute the missing values by zero."},{"metadata":{"collapsed":true,"_cell_guid":"84751314-64dc-4ab1-a588-c918decc0f52","_uuid":"60e1122da40cd92505f5e52d7539be34fbffab75","trusted":false},"cell_type":"code","source":"#imputing missing values\nalldata[\"MasVnrArea\"].fillna(0, inplace=True)\nalldata[\"BsmtFinSF1\"].fillna(0, inplace=True)\nalldata[\"BsmtFinSF2\"].fillna(0, inplace=True)\nalldata[\"BsmtUnfSF\"].fillna(0, inplace=True)\nalldata[\"TotalBsmtSF\"].fillna(0, inplace=True)\nalldata[\"GarageArea\"].fillna(0, inplace=True)\nalldata[\"BsmtFullBath\"].fillna(0, inplace=True)\nalldata[\"BsmtHalfBath\"].fillna(0, inplace=True)\nalldata[\"GarageCars\"].fillna(0, inplace=True)\nalldata[\"GarageYrBlt\"].fillna(0.0, inplace=True)\nalldata[\"PoolArea\"].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"038ab8cb-015b-4dae-81ff-19b6bd5ab6e7","_uuid":"4a65355177325231ede3fb4478dc243b2374ffea"},"cell_type":"markdown","source":"   Variable names which have 'quality' or 'qual' in their names can be treated as ordinal variables, as mentioned above. Now, we'll convert the categorical variables into ordinal variables. To do this, we'll simply create a dictionary of key-value pairs and map it to the variable in the data set."},{"metadata":{"collapsed":true,"_cell_guid":"573ff99b-eeee-4a21-93a8-5fa79d25e119","_uuid":"902fca8628aa2edc12f61c246afca5f5e602abb0","trusted":false},"cell_type":"code","source":"qual_dict = {np.nan: 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\nname = np.array(['ExterQual','PoolQC' ,'ExterCond','BsmtQual','BsmtCond','HeatingQC'\n                 ,'KitchenQual','FireplaceQu', 'GarageQual','GarageCond'])\n\nfor i in name:\n     alldata[i] = alldata[i].map(qual_dict).astype(int)\n\nalldata[\"BsmtExposure\"] = alldata[\"BsmtExposure\"].map({np.nan: 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}).astype(int)\n\nbsmt_fin_dict = {np.nan: 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\nalldata[\"BsmtFinType1\"] = alldata[\"BsmtFinType1\"].map(bsmt_fin_dict).astype(int)\nalldata[\"BsmtFinType2\"] = alldata[\"BsmtFinType2\"].map(bsmt_fin_dict).astype(int)\nalldata[\"Functional\"] = alldata[\"Functional\"].map({np.nan: 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8}).astype(int)\n\nalldata[\"GarageFinish\"] = alldata[\"GarageFinish\"].map({np.nan: 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}).astype(int)\nalldata[\"Fence\"] = alldata[\"Fence\"].map({np.nan: 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}).astype(int)\n\n#encoding data\nalldata[\"CentralAir\"] = (alldata[\"CentralAir\"] == \"Y\") * 1.0 # Convert yes, no to 0 and 1\nvarst = np.array(['MSSubClass','LotConfig','Neighborhood','Condition1'\n                  ,'BldgType','HouseStyle','RoofStyle','Foundation','SaleCondition'])\n\nfor x in varst:\n         factorize(alldata, x)\n\n#encode variables and impute missing values\nalldata = factorize(alldata, \"MSZoning\", \"RL\")\nalldata = factorize(alldata, \"Exterior1st\", \"Other\")\nalldata = factorize(alldata, \"Exterior2nd\", \"Other\")\nalldata = factorize(alldata, \"MasVnrType\", \"None\")\nalldata = factorize(alldata, \"SaleType\", \"Oth\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d598ab07-338e-4da1-86da-d406aee55e1c","_uuid":"56c1ef80e30cda7309974a477ddfae8234076059"},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"_cell_guid":"91d412d3-fa72-4a29-b77f-d398af884c02","_uuid":"6e3a5996a29f04f8c788ed6a566ce4c64b0181dc"},"cell_type":"markdown","source":"There are no libraries or sets of functions you can use to engineer features. Well, there are some but not as effective. It's majorly a manual task but believe me, it's fun. Feature engineering requires domain knowledge and lots of creative ideas. The ideas for new features usually develop during the data exploration and hypothesis generation stages. The motive of feature engineering is to create new features which can help make predictions better.\n\nAs you can also see, there's a massive scope of feature engineering in this data set. Now let's create new features from the given list of 81 features. Make sure you follow this section carefully.\n\nMost categorical variables have near-zero variance distribution. Near-zero variance distribution is when one of the categories in a variable has >90% of the values. We'll create some binary variables depicting the presence or absence of a category. The new features will contain 0 or 1 values. In addition, we'll create some more variables which are self-explanatory with comments."},{"metadata":{"collapsed":true,"_cell_guid":"ad95faab-ca1b-4749-8243-6ed8bfdcac67","_uuid":"c35e4a56c8d9b8891e1d1fe64a3a2789cd52b822","trusted":false},"cell_type":"code","source":"#creating new variable (1 or 0) based on irregular count levels\n#The level with highest count is kept as 1 and rest as 0\nalldata[\"IsRegularLotShape\"] = (alldata[\"LotShape\"] == \"Reg\") * 1\nalldata[\"IsLandLevel\"] = (alldata[\"LandContour\"] == \"Lvl\") * 1\nalldata[\"IsLandSlopeGentle\"] = (alldata[\"LandSlope\"] == \"Gtl\") * 1\nalldata[\"IsElectricalSBrkr\"] = (alldata[\"Electrical\"] == \"SBrkr\") * 1\nalldata[\"IsGarageDetached\"] = (alldata[\"GarageType\"] == \"Detchd\") * 1\nalldata[\"IsPavedDrive\"] = (alldata[\"PavedDrive\"] == \"Y\") * 1\nalldata[\"HasShed\"] = (alldata[\"MiscFeature\"] == \"Shed\") * 1\nalldata[\"Remodeled\"] = (alldata[\"YearRemodAdd\"] != alldata[\"YearBuilt\"]) * 1\n\n#Did the modeling happen during the sale year?\nalldata[\"RecentRemodel\"] = (alldata[\"YearRemodAdd\"] == alldata[\"YrSold\"]) * 1\n\n# Was this house sold in the year it was built?\nalldata[\"VeryNewHouse\"] = (alldata[\"YearBuilt\"] == alldata[\"YrSold\"]) * 1\nalldata[\"Has2ndFloor\"] = (alldata[\"2ndFlrSF\"] == 0) * 1\nalldata[\"HasMasVnr\"] = (alldata[\"MasVnrArea\"] == 0) * 1\nalldata[\"HasWoodDeck\"] = (alldata[\"WoodDeckSF\"] == 0) * 1\nalldata[\"HasOpenPorch\"] = (alldata[\"OpenPorchSF\"] == 0) * 1\nalldata[\"HasEnclosedPorch\"] = (alldata[\"EnclosedPorch\"] == 0) * 1\nalldata[\"Has3SsnPorch\"] = (alldata[\"3SsnPorch\"] == 0) * 1\nalldata[\"HasScreenPorch\"] = (alldata[\"ScreenPorch\"] == 0) * 1\n\n#setting levels with high count as 1 and the rest as 0\n#you can check for them using the value_counts function\nalldata[\"HighSeason\"] = alldata[\"MoSold\"].replace({1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0})\nalldata[\"NewerDwelling\"] = alldata[\"MSSubClass\"].replace({20: 1, 30: 0, 40: 0, 45: 0,50: 0, 60: 1, 70: 0, 75: 0, 80: 0, 85: 0,90: 0, 120: 1, 150: 0, 160: 0, 180: 0, 190: 0})","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"a9acfd9d-6014-4693-8510-5c58337b53ac","_uuid":"c8ddf3fd8ce72fa3cf9acf2e99830bc1f6be6503","trusted":false},"cell_type":"code","source":"#Now, let's check the number of resultant columns.\nalldata.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d4f8d0a1-54e0-4eb0-8590-54a5f3571122","_uuid":"409281ba569e28006fb665f14d83c25832fa9e93"},"cell_type":"markdown","source":"Now, we have 100 features in the data. It means we create 19 more columns. Let's continue and create some more features. Once again, we'll combine the original train and test files to create a parallel alldata2 file. This file will have original feature values. We'll use this data as reference to create more features.\n\n"},{"metadata":{"collapsed":true,"_cell_guid":"f901f9d9-ddd6-4d74-a166-d7ba4152993f","_uuid":"bfd23e64283170e55dc4d9f16ce5d491342dc3bf","trusted":false},"cell_type":"code","source":"#create alldata2\nalldata2 = train.append(test)\n\nalldata[\"SaleCondition_PriceDown\"] = alldata2.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n\n# house completed before sale or not\nalldata[\"BoughtOffPlan\"] = alldata2.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\nalldata[\"BadHeating\"] = alldata2.HeatingQC.replace({'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4060b10d-0687-46ad-bffc-baf8917c01a0","_uuid":"d51a1aaea57b83d7cd1ae6df304e12936d1b4aec"},"cell_type":"markdown","source":"Just like Garage, we have several columns associated with the area of the property. An interesting variable could be the sum of all areas for a particular house. In addition, we can also create new features based on the year the house built."},{"metadata":{"collapsed":true,"_cell_guid":"7f6fbcd2-d26a-46a1-8a84-f72f051338b0","_uuid":"ae91cdd27ced812f04be6a05a00ad3d99786b230","trusted":false},"cell_type":"code","source":"#calculating total area using all area columns\narea_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF'\n             , '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF'\n             ,'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'LowQualFinSF', 'PoolArea' ]\n\nalldata[\"TotalArea\"] = alldata[area_cols].sum(axis=1)\nalldata[\"TotalArea1st2nd\"] = alldata[\"1stFlrSF\"] + alldata[\"2ndFlrSF\"]\nalldata[\"Age\"] = 2010 - alldata[\"YearBuilt\"]\nalldata[\"TimeSinceSold\"] = 2010 - alldata[\"YrSold\"]\nalldata[\"SeasonSold\"] = alldata[\"MoSold\"].map({12:0, 1:0, 2:0, 3:1, 4:1, 5:1, 6:2, 7:2, 8:2, 9:3, 10:3, 11:3}).astype(int)\nalldata[\"YearsSinceRemodel\"] = alldata[\"YrSold\"] - alldata[\"YearRemodAdd\"]\n\n# Simplifications of existing features into bad/average/good based on counts\nalldata[\"SimplOverallQual\"] = alldata.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\nalldata[\"SimplOverallCond\"] = alldata.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\nalldata[\"SimplPoolQC\"] = alldata.PoolQC.replace({1 : 1, 2 : 1, 3 : 2, 4 : 2})\nalldata[\"SimplGarageCond\"] = alldata.GarageCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplGarageQual\"] = alldata.GarageQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFireplaceQu\"] = alldata.FireplaceQu.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFireplaceQu\"] = alldata.FireplaceQu.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplFunctional\"] = alldata.Functional.replace({1 : 1, 2 : 1, 3 : 2, 4 : 2, 5 : 3, 6 : 3, 7 : 3, 8 : 4})\nalldata[\"SimplKitchenQual\"] = alldata.KitchenQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplHeatingQC\"] = alldata.HeatingQC.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplBsmtFinType1\"] = alldata.BsmtFinType1.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\nalldata[\"SimplBsmtFinType2\"] = alldata.BsmtFinType2.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\nalldata[\"SimplBsmtCond\"] = alldata.BsmtCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplBsmtQual\"] = alldata.BsmtQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplExterCond\"] = alldata.ExterCond.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\nalldata[\"SimplExterQual\"] = alldata.ExterQual.replace({1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n\n#grouping neighborhood variable based on this plot\ntrain['SalePrice'].groupby(train['Neighborhood']).median().sort_values().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d29935bd-c1ec-4d01-a1e2-b4cf1fe9bddf","_uuid":"42a1a02e8e2bb73aaffa08b798e8302354d31968"},"cell_type":"markdown","source":"The graph above gives us a good hint on how to combine levels of the neighborhood variable into fewer levels. We can combine bars of somewhat equal height in one category. To do this, we'll simply create a dictionary and map it with variable values."},{"metadata":{"collapsed":true,"_cell_guid":"fb85669a-ceb0-4b4e-8638-e3f179f1d513","_uuid":"8c2219fca1e35739344d4c7c47453a7df169d462","trusted":false},"cell_type":"code","source":"neighborhood_map = {\"MeadowV\" : 0, \"IDOTRR\" : 1, \"BrDale\" : 1, \"OldTown\" : 1\n                    , \"Edwards\" : 1, \"BrkSide\" : 1, \"Sawyer\" : 1, \"Blueste\" : 1\n                    , \"SWISU\" : 2, \"NAmes\" : 2, \"NPkVill\" : 2, \"Mitchel\" : 2\n                    , \"SawyerW\" : 2, \"Gilbert\" : 2, \"NWAmes\" : 2, \"Blmngtn\" : 2\n                    , \"CollgCr\" : 2, \"ClearCr\" : 3, \"Crawfor\" : 3, \"Veenker\" : 3\n                    , \"Somerst\" : 3, \"Timber\" : 3, \"StoneBr\" : 4, \"NoRidge\" : 4\n                    , \"NridgHt\" : 4}\n\nalldata['NeighborhoodBin'] = alldata2['Neighborhood'].map(neighborhood_map)\nalldata.loc[alldata2.Neighborhood == 'NridgHt', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'Crawfor', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'StoneBr', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'Somerst', \"Neighborhood_Good\"] = 1\nalldata.loc[alldata2.Neighborhood == 'NoRidge', \"Neighborhood_Good\"] = 1\nalldata[\"Neighborhood_Good\"].fillna(0, inplace=True)\nalldata[\"SaleCondition_PriceDown\"] = alldata2.SaleCondition.replace({'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n\n# House completed before sale or not\nalldata[\"BoughtOffPlan\"] = alldata2.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\nalldata[\"BadHeating\"] = alldata2.HeatingQC.replace({'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})\nalldata.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e8ce3cb-e1ba-454e-ab22-5700da290b83","_uuid":"8871b5ad4fde46dbe3d049b835a7dfe0b151e406"},"cell_type":"markdown","source":"Until this point, we've added 43 new features in the data set. Now, let's split the data into test and train and create some more features."},{"metadata":{"collapsed":true,"_cell_guid":"cfe59e52-963f-4d43-95cf-f927e2c8dcd0","_uuid":"ac8657f27bb15cc056e71a167e577b05d6fdf007","trusted":false},"cell_type":"code","source":"#create new data\ntrain_new = alldata[alldata['SalePrice'].notnull()]\ntest_new = alldata[alldata['SalePrice'].isnull()]\n\nprint(\"Train\", train_new.shape)\nprint ('----------------')\nprint(\"Test\", test_new.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ddd8c818-2590-4b84-b703-42bb65df1e5d","_uuid":"d29024d4e168b23dc485eda6fb9cf38838692f07"},"cell_type":"markdown","source":"Now, we'll transform numeric features and remove their skewness."},{"metadata":{"collapsed":true,"_cell_guid":"ecee6885-b621-474d-b2f9-618468dfd6a5","_uuid":"32bc4dff9c7732fbc5a647f5b254db5cd5225d25","trusted":false},"cell_type":"code","source":"#get numeric features\nnumeric_features = [f for f in train_new.columns if train_new[f].dtype != object]\n\n#transform the numeric features using log(x + 1)\nfrom scipy.stats import skew\nskewed = train_new[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))\nskewed = skewed[skewed > 0.75]\nskewed = skewed.index\ntrain_new[skewed] = np.log1p(train_new[skewed])\ntest_new[skewed] = np.log1p(test_new[skewed])\ntest_label =  test_new['SalePrice']\ndel test_new['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de2eb2f5-9000-49a0-8501-6bc2e5cf8b5d","_uuid":"6ec7cdb1880e78549d23ba2cc9e54e80b7b6e3e0"},"cell_type":"markdown","source":"Now, we'll standardize the numeric features."},{"metadata":{"collapsed":true,"_cell_guid":"14c31ce4-962b-4f0d-a5cf-a8a5165fc7f1","_uuid":"5bd18f87ecf4c1823085aa36f84f8e331a7d7eee","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(train_new[numeric_features])\nscaled = scaler.transform(train_new[numeric_features])\n\nfor i, col in enumerate(numeric_features):\n       train_new[col] = scaled[:,i]\n\nnumeric_features.remove('SalePrice')\nscaled = scaler.fit_transform(test_new[numeric_features])\n\nfor i, col in enumerate(numeric_features):\n      test_new[col] = scaled[:,i]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c68cb8be-8695-40ae-941f-68ea4acc915a","_uuid":"c267d6d8f49c9a3312a7fc5570ad9672a44f6585"},"cell_type":"markdown","source":"Now, we'll one-hot encode the categorical variable. In one-hot encoding, every level of a categorical variable results in a new variable with binary values (0 or 1). We'll write a function to encode categorical variables:"},{"metadata":{"collapsed":true,"_cell_guid":"c4a5e251-8251-4dd8-a6d4-9dca62780064","_uuid":"e63fb870b4b889658a3789db0b9cbe00c7a542ef","trusted":false},"cell_type":"code","source":"def onehot(onehot_df, df, column_name, fill_na):\n       onehot_df[column_name] = df[column_name]\n       if fill_na is not None:\n            onehot_df[column_name].fillna(fill_na, inplace=True)\n\n       dummies = pd.get_dummies(onehot_df[column_name], prefix=\"_\"+column_name)\n       onehot_df = onehot_df.join(dummies)\n       onehot_df = onehot_df.drop([column_name], axis=1)\n       return onehot_df\n\ndef munge_onehot(df):\n       onehot_df = pd.DataFrame(index = df.index)\n\n       onehot_df = onehot(onehot_df, df, \"MSSubClass\", None)\n       onehot_df = onehot(onehot_df, df, \"MSZoning\", \"RL\")\n       onehot_df = onehot(onehot_df, df, \"LotConfig\", None)\n       onehot_df = onehot(onehot_df, df, \"Neighborhood\", None)\n       onehot_df = onehot(onehot_df, df, \"Condition1\", None)\n       onehot_df = onehot(onehot_df, df, \"BldgType\", None)\n       onehot_df = onehot(onehot_df, df, \"HouseStyle\", None)\n       onehot_df = onehot(onehot_df, df, \"RoofStyle\", None)\n       onehot_df = onehot(onehot_df, df, \"Exterior1st\", \"VinylSd\")\n       onehot_df = onehot(onehot_df, df, \"Exterior2nd\", \"VinylSd\")\n       onehot_df = onehot(onehot_df, df, \"Foundation\", None)\n       onehot_df = onehot(onehot_df, df, \"SaleType\", \"WD\")\n       onehot_df = onehot(onehot_df, df, \"SaleCondition\", \"Normal\")\n\n       #Fill in missing MasVnrType for rows that do have a MasVnrArea.\n       temp_df = df[[\"MasVnrType\", \"MasVnrArea\"]].copy()\n       idx = (df[\"MasVnrArea\"] != 0) & ((df[\"MasVnrType\"] == \"None\") | (df[\"MasVnrType\"].isnull()))\n       temp_df.loc[idx, \"MasVnrType\"] = \"BrkFace\"\n       onehot_df = onehot(onehot_df, temp_df, \"MasVnrType\", \"None\")\n\n       onehot_df = onehot(onehot_df, df, \"LotShape\", None)\n       onehot_df = onehot(onehot_df, df, \"LandContour\", None)\n       onehot_df = onehot(onehot_df, df, \"LandSlope\", None)\n       onehot_df = onehot(onehot_df, df, \"Electrical\", \"SBrkr\")\n       onehot_df = onehot(onehot_df, df, \"GarageType\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"PavedDrive\", None)\n       onehot_df = onehot(onehot_df, df, \"MiscFeature\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Street\", None)\n       onehot_df = onehot(onehot_df, df, \"Alley\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Condition2\", None)\n       onehot_df = onehot(onehot_df, df, \"RoofMatl\", None)\n       onehot_df = onehot(onehot_df, df, \"Heating\", None)\n\n       # we'll have these as numerical variables too\n       onehot_df = onehot(onehot_df, df, \"ExterQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"ExterCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"HeatingQC\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"KitchenQual\", \"TA\")\n       onehot_df = onehot(onehot_df, df, \"FireplaceQu\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"GarageQual\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"GarageCond\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"PoolQC\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtExposure\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtFinType1\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"BsmtFinType2\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Functional\", \"Typ\")\n       onehot_df = onehot(onehot_df, df, \"GarageFinish\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"Fence\", \"None\")\n       onehot_df = onehot(onehot_df, df, \"MoSold\", None)\n\n       # Divide  the years between 1871 and 2010 into slices of 20 years\n       year_map = pd.concat(pd.Series(\"YearBin\" + str(i+1), index=range(1871+i*20,1891+i*20))  for i in range(0, 7))\n       yearbin_df = pd.DataFrame(index = df.index)\n       yearbin_df[\"GarageYrBltBin\"] = df.GarageYrBlt.map(year_map)\n       yearbin_df[\"GarageYrBltBin\"].fillna(\"NoGarage\", inplace=True)\n       yearbin_df[\"YearBuiltBin\"] = df.YearBuilt.map(year_map)\n       yearbin_df[\"YearRemodAddBin\"] = df.YearRemodAdd.map(year_map)\n\n       onehot_df = onehot(onehot_df, yearbin_df, \"GarageYrBltBin\", None)\n       onehot_df = onehot(onehot_df, yearbin_df, \"YearBuiltBin\", None)\n       onehot_df = onehot(onehot_df, yearbin_df, \"YearRemodAddBin\", None)\n       return onehot_df\n\n#create one-hot features\nonehot_df = munge_onehot(train)\n\nneighborhood_train = pd.DataFrame(index=train_new.shape)\nneighborhood_train['NeighborhoodBin'] = train_new['NeighborhoodBin']\nneighborhood_test = pd.DataFrame(index=test_new.shape)\nneighborhood_test['NeighborhoodBin'] = test_new['NeighborhoodBin']\n\nonehot_df = onehot(onehot_df, neighborhood_train, 'NeighborhoodBin', None)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d1483002-fb82-42f7-8e85-30089ba48693","_uuid":"f7130d36db9ebefb870918199bc325aa062c996e"},"cell_type":"markdown","source":"Let's add the one-hot variables in our train data set."},{"metadata":{"collapsed":true,"_cell_guid":"3552f00a-7351-4d42-86db-8203dde47745","_uuid":"17406594659c5b23faa6ddde89fe800bb75e4eda","trusted":false},"cell_type":"code","source":"train_new = train_new.join(onehot_df) \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ec1c31fb-0bef-4367-86b0-aadf57b7c2b6","_uuid":"83320d2525ec8a1ed45b03fe654891ae0e1ac6f6"},"cell_type":"markdown","source":"Woah! This resulted in a whopping 433 columns. Similarly, we will add one-hot variables in test data as well."},{"metadata":{"collapsed":true,"_cell_guid":"b2208734-2e5a-4b07-b958-55831362e4eb","_uuid":"706e96f324208d28090d275cfa19ae42bf654db9","trusted":false},"cell_type":"code","source":"#adding one hot features to test\nonehot_df_te = munge_onehot(test)\nonehot_df_te = onehot(onehot_df_te, neighborhood_test, \"NeighborhoodBin\", None)\ntest_new = test_new.join(onehot_df_te)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96aada82-d781-49b3-8ce2-636fbc1360ab","_uuid":"161fa14875bdeaf0800a1eb7749401ceefaff3eb"},"cell_type":"markdown","source":"The difference in number of train and test columns suggests that some new features in the train data aren't available in the test data. Let's remove those variables and keep an equal number of columns in the train and test data."},{"metadata":{"collapsed":true,"_cell_guid":"671fd23b-9cb6-4359-94bf-fef23733b816","_uuid":"9c38ff634d94c4d96bfb4015478d195b401572b2","trusted":false},"cell_type":"code","source":"#dropping some columns from the train data as they are not found in test\ndrop_cols = [\"_Exterior1st_ImStucc\", \"_Exterior1st_Stone\",\"_Exterior2nd_Other\",\"_HouseStyle_2.5Fin\",\"_RoofMatl_Membran\", \"_RoofMatl_Metal\", \"_RoofMatl_Roll\", \"_Condition2_RRAe\", \"_Condition2_RRAn\", \"_Condition2_RRNn\", \"_Heating_Floor\", \"_Heating_OthW\", \"_Electrical_Mix\", \"_MiscFeature_TenC\", \"_GarageQual_Ex\",  \"_PoolQC_Fa\"]\ntrain_new.drop(drop_cols, axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"94ce404b-b78c-4850-8c62-1bf96a06f5b8","_uuid":"dcf86ce628ebbfe630c5f96a40d56cca45801e44"},"cell_type":"markdown","source":"Now, we have an equal number of columns in the train and test data. Here, we'll remove a few more columns which either have lots of zeroes (hence doesn't provide any real information) or aren't available in either of the data sets."},{"metadata":{"collapsed":true,"_cell_guid":"15900808-e9f7-435b-8835-7420acfc2c78","_uuid":"e614e1f3d01c914c903b19dc14a37e872a057131","trusted":false},"cell_type":"code","source":"#removing one column missing from train data\ntest_new.drop([\"_MSSubClass_150\"], axis=1, inplace=True)\n\n# Drop these columns\ndrop_cols = [\"_Condition2_PosN\", # only two are not zero\n         \"_MSZoning_C (all)\",\n         \"_MSSubClass_160\"]\n\ntrain_new.drop(drop_cols, axis=1, inplace=True)\ntest_new.drop(drop_cols, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fb7da92-26ae-4962-87d3-7581961ab363","_uuid":"4a8ed55baf227b6b74d43535cc8cdacf1ef089f3"},"cell_type":"markdown","source":"Let's transform the target variable and store it in a new array."},{"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"71a45ad1-5c10-43c4-b0b7-1215b53f390f","_uuid":"5dec46a71d4a3f31066160630c15e9d64b5b5b42","trusted":false},"cell_type":"code","source":"#create a label set\nlabel_df = pd.DataFrame(index = train_new.index, columns = ['SalePrice'])\nlabel_df['SalePrice'] = np.log(train['SalePrice'])\nprint(\"Training set size:\", train_new.shape)\nprint(\"Test set size:\", test_new.shape)\n#print(label_df.head)\nprint(test_label)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"d893d6ca-b631-474b-87d8-8b38fd6f0905","_uuid":"fa7d2e8d8dd21773b24661e36e802395766fb9d3","trusted":false},"cell_type":"code","source":"test_label = test_new['SalePrice']\nprint(test_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aef6d149-d45a-4898-9fd1-f53c1f6de105","_uuid":"44bcddb394af35ef37eafb59c0aac912ba662a3b"},"cell_type":"markdown","source":"# Model Training and Evaluation"},{"metadata":{"_cell_guid":"86e4ad28-5e4d-470a-acce-3472cd2b81f9","_uuid":"ee61b2f937bcd712bd9d87b225b54209b92c47b4"},"cell_type":"markdown","source":"Since our data is ready, we'll start training models now. We'll use three algorithms: XGBoost, Neural Network and Lasso Regression. Finally, we'll ensemble the models to generate final predictions."},{"metadata":{"collapsed":true,"_cell_guid":"367165c9-60a2-48f2-804a-300e1447ebf5","_uuid":"59af723fcba58c30698f5e00cf491cfd97d30dd2","trusted":false},"cell_type":"code","source":"train_new = train_new.select_dtypes(include=[np.number]) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"b8d166ff-6434-4308-86d0-5257fb0d3acb","_uuid":"38c64d0f06dc170cffb4504d450b39705050eac2","trusted":false},"cell_type":"code","source":"print(train_new.columns)\nprint(test_new.columns)\ndef intersect(a, b):\n    return list(set(a) & set(b))\n\ninter_cols = intersect(test_new.columns.tolist(), train_new.columns.tolist()) ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"fbc432a5-8992-4483-83f4-f67e17b0b225","_uuid":"43be113ca8b0fccbc4a705e0a73209bf86776704","trusted":false},"cell_type":"code","source":"import xgboost as xgb\nregr = xgb.XGBRegressor(colsample_bytree=0.2,\n                       gamma=0.0,\n                       learning_rate=0.05,\n                       max_depth=6,\n                       min_child_weight=1.5,\n                       n_estimators=7200,\n                       reg_alpha=0.9,\n                       reg_lambda=0.6,\n                       subsample=0.2,\n                       seed=42,\n                       silent=1)\n\nregr.fit(train_new[inter_cols], label_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1a8608c-71e0-4645-bda8-23ce1ed0e86a","_uuid":"7c6b945d922d4eed0f60c9c75444530c533a8337"},"cell_type":"markdown","source":"These parameters’ values are derived using cross-validation. To evaluate the model's performance, we'll create a quick RMSE function."},{"metadata":{"collapsed":true,"_cell_guid":"3a17532f-89d9-4159-ac93-6a47e22f5a25","_uuid":"1427063dc06a86a1bc43cbf7b9a28c75fa7b2023","trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ndef rmse(y_test,y_pred):\n      return np.sqrt(mean_squared_error(y_test,y_pred))\n\n# run prediction on training set to get an idea of how well it does\ny_pred = regr.predict(train_new[inter_cols])\ny_test = label_df\nprint(\"XGBoost score on training set: \", rmse(y_test, y_pred))\nprint(y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2f9719b7-5039-4968-b365-1ef46b2d09d4","_uuid":"b251ea1896f3e96f02ada34aaef054f863016548","trusted":false},"cell_type":"code","source":"#test_new = test_new.select_dtypes(include=[np.number])\n# make prediction on test set\ny_pred_xgb = regr.predict(kk)\n\n#submit this prediction and get the score\npred1 = pd.DataFrame({'Id': test['Id'], 'SalePrice': np.exp(y_pred_xgb)}) # exp = Antilog of SalesProce\npred1.to_csv('xgbnono.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"1b273585-4270-405d-9649-424e1be1c226","_uuid":"52ccc1ec7428a264ee0f3c0ab1c319fff07417cc","trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\n#found this best alpha through cross-validation\n#best_alpha = 0.00099\n\nregr = Lasso(alpha=best_alpha, max_iter=50000)\nregr.fit(train_new[inter_cols], label_df)\n\n# run prediction on the training set to get a rough idea of how well it does\ny_pred = regr.predict(train_new[inter_cols])\ny_test = label_df\nprint(\"Lasso score on training set: \", rmse(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e4e2fd40-ea26-4171-a805-8a0f3b8a640e","_uuid":"50c4a4b7429a634609e0c63aa40261452f7e7bbb","trusted":false},"cell_type":"code","source":"#make prediction on the test set\ny_pred_lasso = regr.predict(test_new[inter_cols])\nlasso_ex = np.exp(y_pred_lasso)\nprint(\"Lasso score on training set: \", rmse(y_test, y_pred_lasso))\npred1 = pd.DataFrame({'Id': test['Id'], 'SalePrice': lasso_ex})\npred1.to_csv('lasso_model.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"541e9a92cf6bede14c8b875068425df4fc8b7e1e"},"cell_type":"code","source":"# 3* Lasso\nfrom sklearn.linear_model import LassoCV\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(train_new[inter_cols], label_df)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(train_new[inter_cols], label_df)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\n#make prediction on the test set\ny_pred_lasso = lasso.predict(test_new[inter_cols])\nlasso_ex = np.exp(y_pred_lasso)\nprint(\"Lasso score on training set: \", rmse(y_test, y_pred_lasso))\npred1 = pd.DataFrame({'Id': test['Id'], 'SalePrice': lasso_ex})\npred1.to_csv('lasso_model2.csv', header=True, index=False)","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"2dd83082-71f6-49fb-953d-3b0bc668ddb7","_uuid":"c02063ee1835d7abff31e9cb9fb65cbecf7b51b9"},"cell_type":"markdown","source":"Let's upload this file on Kaggle and check the score. We scored 0.11859 on the leaderboard. We see that the lasso model has outperformed the formidable XGBoost algorithm. And, it also improved our rank. With this, we have advanced into the top 16% of the participants. Since the data set is high dimensional (means large number of features), we can try our hands at building a neural network model as well. Let's do it. We'll use the keras library to train the neural network."},{"metadata":{"scrolled":true,"collapsed":true,"_cell_guid":"079a7037-8e16-45f5-aaf5-7975ae52dcf5","_uuid":"bc1a2a7f3defd0460a0a80389fa4aaa318495110","trusted":false},"cell_type":"code","source":"from keras.models import Sequential # Data flows in only one direction\nfrom keras.layers import Dense #Every node is connected to next layer's every node\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.preprocessing import StandardScaler # Scaling z = x-mean(x)/(std(x))\n\nnp.random.seed(10)\n\n#create Model\n#define base model\ndef base_model():\n     model = Sequential()\n     model.add(Dense(80, input_dim=398, kernel_initializer='normal', activation='relu')) # first layer\n     model.add(Dense(60, kernel_initializer='normal', activation='relu')) # second and hidden layer\n     #model.add(Dense(40, kernel_initializer='normal', activation='relu')) # second and hidden layer\n     model.add(Dense(20, kernel_initializer='normal', activation='relu')) # second and hidden layer\n     model.add(Dense(1, init='normal')) # 3rd and output layer\n     model.compile(loss='mean_squared_error', optimizer = 'adam', ) #Minimize the MSE using adam\n     return model\n\nseed = 7\nnp.random.seed(seed)\n\nscale = StandardScaler()\nX_train = scale.fit_transform(train_new[inter_cols]) # Scaling\nX_test = scale.fit_transform(test_new[inter_cols]) # Scaling\n\nkeras_label = label_df.as_matrix()\nclf = KerasRegressor(build_fn=base_model, nb_epoch=1000, batch_size=10,verbose=0, epochs = 50)\nclf.fit(X_train,keras_label)\n\n#make predictions and create the submission file \nkpred = clf.predict(X_test) \nkpred = np.exp(kpred)\npred_df = pd.DataFrame(kpred, index=test[\"Id\"], columns=[\"SalePrice\"]) \npred_df.to_csv('keras1.csv', header=True, index_label='Id') \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"0161a0c4-8777-4710-bb1a-56ee7330caf3","_uuid":"6ffc4c035e7603473bfe0403227cba793c896fa4","trusted":false},"cell_type":"code","source":"print(\"Neural Network score on training set: \", rmse(y_test, kpred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce5c221b-0de0-4332-b47e-bab93d9c1d6f","_uuid":"1c593555dcfd941b10466ddb35decb0438194e61"},"cell_type":"markdown","source":"Let's submit this file and check our score. We get RMSE as 1.35346, which is worse than the previous two models. We drop this model here. For further improvement, let's try to ensemble the predictions from XGBoost and lasso model. We'll simply average the predictions."},{"metadata":{"collapsed":true,"_cell_guid":"6b7c34da-2182-4470-a9e0-56c5589e9be4","_uuid":"23da34c59f478a3e2f0745936ef0c7012b15a8a7","trusted":false},"cell_type":"code","source":"#simple average\ny_pred = (y_pred_xgb + y_pred_lasso) / 2\ny_pred = np.exp(y_pred)\npred_df = pd.DataFrame(y_pred, index=test[\"Id\"], columns=[\"SalePrice\"])\npred_df.to_csv('ensemble1.csv', header=True, index_label='Id')\nprint(pred_df)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"c1038de0-ae3a-4441-9137-62ef8ec6c9e4","_uuid":"f66d9f72e253cde29e2c14f0078927a4c30ead7c","trusted":false},"cell_type":"code","source":"x = y_pred\ny = kpred\n#plt.plot(act,pred,\"o\")\n\nfit = np.polyfit(x,y,1)\nfit_fn = np.poly1d(fit) \n# fit_fn is now a function which takes in x and returns an estimate for y\n\nplt.plot(x,y, 'o', x, fit_fn(x), '--k')\n\nfrom pydoc import help\nfrom scipy.stats.stats import pearsonr\n#help(pearsonr)\npearsonr(x, y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"432a01ed-ae7c-4284-8788-7100ed9d99c7","_uuid":"afcc655242fc1d052da39f169feb22f72dca42b9"},"cell_type":"markdown","source":"Let's submit this file and check our score. We get RMSE as 0.11792, which is our best score so far. This puts us in top 14% of the participants. Often, ensemble technique outperforms the single best model. With this, we come to the end of this tutorial. But, you shouldn't stop here. Keep improving the accuracy by trying different algorithms and tune their parameters."},{"metadata":{"_cell_guid":"bafde932-a166-4b2d-939a-68f284dfc6db","_uuid":"04f2d629459fbd3c1ff7086f1c4bc05f489b1ce4"},"cell_type":"markdown","source":"elasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\nprint(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\ny_train_ela = elasticNet.predict(X_train)\ny_test_ela = elasticNet.predict(X_test)\n"},{"metadata":{"_cell_guid":"9dfc8773-33f8-4dcd-8d6b-d5a2666cc637","_uuid":"a8608b7923fb5ad3f17d229a79eaccea8553ae5a"},"cell_type":"markdown","source":"# Summary\nThe project has been created to help people understand the complete process of machine learning / data science modeling. These steps ensure that you won't miss out any information in the data set and would also help another person understand your work. I would like to thank the Kaggle community for sharing info on competition forums which helped me a lot in creating this tutorial."},{"metadata":{"_cell_guid":"7799cab2-708e-43ee-9284-178cf8701394","_uuid":"3af1f71c2c930968c463408808e41716430f6b6c"},"cell_type":"markdown","source":"Ref. https://www.hackerearth.com/practice/machine-learning/machine-learning-projects/python-project/tutorial/"},{"metadata":{"collapsed":true,"_cell_guid":"fbe3c32c-5d22-4d20-a38b-5fc6653dfa1a","_uuid":"afd7cab035447736549b694493a610bfdfc4c345","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}